{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAM 3 Agent (Deployment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows an example of how an MLLM can use SAM 3 as a tool, i.e., \"SAM 3 Agent\", to segment more complex text queries such as \"the leftmost child wearing blue vest\".\n",
        "\n",
        "**This version uses:**\n",
        "- **Local vLLM** for LLM calls (same as original notebook)\n",
        "- **Deployed SAM3 service** (Modal endpoint) for SAM3 inference instead of loading the model locally\n",
        "- **Local agent logic** that orchestrates between local vLLM and remote SAM3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First install `sam3` in your environment using the [installation instructions](https://github.com/facebookresearch/sam3?tab=readme-ov-file#installation) in the repository.\n",
        "\n",
        "**Note**: Since SAM3 is deployed remotely, you don't need to load the SAM3 model locally. However, you still need the SAM3 package installed for any helper functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# turn on tfloat32 for Ampere GPUs\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "# inference mode for the whole notebook. Disable if you need gradients\n",
        "torch.inference_mode().__enter__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "import requests\n",
        "from IPython.display import display, Image\n",
        "from pathlib import Path\n",
        "\n",
        "SAM3_ROOT = os.path.dirname(os.getcwd())\n",
        "os.chdir(SAM3_ROOT)\n",
        "\n",
        "# setup GPU to use - needed for vLLM if running locally\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "_ = os.system(\"nvidia-smi\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAM3 Deployment Configuration\n",
        "\n",
        "Configure the SAM3 deployment endpoint URL. This should point to your deployed SAM3 service (e.g., Modal deployment).\n",
        "\n",
        "**Note**: The SAM3 model is loaded and running on the deployment server, not locally. We'll use the `/sam3/infer` endpoint for pure SAM3 inference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAM3 Deployment endpoint URL for pure inference (no LLM/agent)\n",
        "# Replace with your Modal deployment URL (e.g., https://your-username--sam3-agent-sam3-infer.modal.run)\n",
        "SAM3_DEPLOYMENT_URL = \"https://srinjoy59--sam3-agent-sam3-infer.modal.run\"\n",
        "\n",
        "# Verify the endpoint is accessible (optional check)\n",
        "print(f\"SAM3 Deployment URL: {SAM3_DEPLOYMENT_URL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Setup\n",
        "\n",
        "Config which MLLM to use, it can either be a model served by vLLM that you launch from your own machine or a model is served via external API. If you want to using a vLLM model, we also provided insturctions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM_CONFIGS = {\n",
        "    # vLLM-served models\n",
        "    \"qwen3_vl_8b_thinking\": {\n",
        "        \"provider\": \"vllm\",\n",
        "        \"model\": \"Qwen/Qwen3-VL-8B-Thinking\",\n",
        "    }, \n",
        "    # models served via external APIs\n",
        "    # add your own\n",
        "}\n",
        "\n",
        "model = \"qwen3_vl_8b_thinking\"\n",
        "LLM_API_KEY = \"DUMMY_API_KEY\"\n",
        "\n",
        "llm_config = LLM_CONFIGS[model]\n",
        "llm_config[\"api_key\"] = LLM_API_KEY\n",
        "llm_config[\"name\"] = model\n",
        "\n",
        "# setup API endpoint\n",
        "if llm_config[\"provider\"] == \"vllm\":\n",
        "    LLM_SERVER_URL = \"http://0.0.0.0:8001/v1\"  # replace this with your vLLM server address as needed\n",
        "else:\n",
        "    LLM_SERVER_URL = llm_config[\"base_url\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup vLLM server \n",
        "This step is only required if you are using a model served by vLLM, skip this step if you are calling LLM using an API like Gemini and GPT.\n",
        "\n",
        "* Install vLLM (in a separate conda env from SAM 3 to avoid dependency conflicts).\n",
        "  ```bash\n",
        "    conda create -n vllm python=3.12\n",
        "    pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128\n",
        "  ```\n",
        "* Start vLLM server on the same machine of this notebook\n",
        "  ```bash\n",
        "    # qwen 3 VL 8B thinking\n",
        "    vllm serve Qwen/Qwen3-VL-8B-Thinking --tensor-parallel-size 4 --allowed-local-media-path / --enforce-eager --port 8001\n",
        "  ```\n",
        "\n",
        "**Note**: Since we're running the agent locally, the vLLM server only needs to be accessible from this notebook (localhost), not from Modal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start vLLM server in the background\n",
        "# Make sure MODEL_ID matches the model in LLM_CONFIGS above\n",
        "MODEL_ID = llm_config[\"model\"]  # e.g., \"Qwen/Qwen3-VL-8B-Thinking\"\n",
        "\n",
        "# Check if vLLM server is already running\n",
        "import subprocess\n",
        "check_result = subprocess.run(\n",
        "    [\"pgrep\", \"-f\", \"vllm serve\"],\n",
        "    capture_output=True,\n",
        "    text=True,\n",
        "    check=False\n",
        ")\n",
        "\n",
        "if check_result.returncode == 0 and check_result.stdout.strip():\n",
        "    print(\"‚ö† vLLM server appears to be already running.\")\n",
        "    print(\"   If you want to restart, stop it first or restart the kernel.\")\n",
        "else:\n",
        "    # The command to run, broken into a list\n",
        "    command = [\n",
        "        \"nohup\",\n",
        "        \"vllm\", \"serve\",\n",
        "        MODEL_ID,\n",
        "        \"--trust-remote-code\",\n",
        "        \"--dtype\", \"bfloat16\", \n",
        "        # \"--max-model-len\", \"65536\",  # Uncomment if needed\n",
        "        \"--gpu-memory-utilization\", \"0.9\",\n",
        "        \"--port\", \"8001\",  # Match the port in LLM_SERVER_URL\n",
        "        \"--host\", \"0.0.0.0\",  # Allow external connections (needed for Modal deployment)\n",
        "    ]\n",
        "    \n",
        "    # Open a file to redirect stdout and stderr\n",
        "    vllm_log = open('vllm.log', 'w')\n",
        "    \n",
        "    process = subprocess.Popen(\n",
        "        command,\n",
        "        stdout=vllm_log,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        preexec_fn=os.setpgrp\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úì vLLM server started in the background with PID: {process.pid}\")\n",
        "    print(\"  Logs are being written to vllm.log\")\n",
        "    print(f\"  Model: {MODEL_ID}\")\n",
        "    print(f\"  Server will be available at: {LLM_SERVER_URL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for vLLM server to become ready\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "LOG_PATH = Path(\"vllm.log\")\n",
        "CHECK_CMD = [\"pgrep\", \"-f\", \"vllm serve\"]\n",
        "TIMEOUT = 1200  # 20 minutes\n",
        "LOG_TAIL_LINES = 50\n",
        "\n",
        "def get_vllm_pids():\n",
        "    \"\"\"Return list of PIDs for 'vllm serve' or [] if none.\"\"\"\n",
        "    result = subprocess.run(\n",
        "        CHECK_CMD,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=False,       \n",
        "    )\n",
        "    if result.returncode != 0:\n",
        "        return []\n",
        "    out = result.stdout.strip()\n",
        "    if not out:\n",
        "        return []\n",
        "    return [int(p) for p in out.split() if p.strip().isdigit()]\n",
        "\n",
        "def log_tail(path: Path, n: int = 50) -> str:\n",
        "    \"\"\"Return last n lines of a log file, or '' if missing.\"\"\"\n",
        "    if not path.exists():\n",
        "        return \"\"\n",
        "    with path.open(\"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    return \"\".join(lines[-n:])\n",
        "\n",
        "# Wait a bit before we start checking\n",
        "time.sleep(5)\n",
        "\n",
        "start = time.time()\n",
        "print(\"Waiting for vLLM server to become ready...\")\n",
        "\n",
        "while True:\n",
        "    diff = time.time() - start\n",
        "    print(f\"Time since start: {diff:.1f} seconds\")\n",
        "\n",
        "    if diff > TIMEOUT:\n",
        "        print(\"‚ö† 20 minutes passed, vLLM server not ready. Exiting monitor loop.\")\n",
        "        break\n",
        "\n",
        "    pids = get_vllm_pids()\n",
        "\n",
        "    if not pids:\n",
        "        # No process at all ‚Üí either never started or crashed.\n",
        "        print(\"‚ùå vLLM server process not found (stopped / failed to start).\")\n",
        "        tail = log_tail(LOG_PATH, LOG_TAIL_LINES)\n",
        "        if tail:\n",
        "            print(\"\\nLast log lines:\\n\" + \"-\" * 60)\n",
        "            print(tail)\n",
        "            print(\"-\" * 60)\n",
        "        break\n",
        "\n",
        "    # Process exists; now inspect logs for startup completion\n",
        "    if LOG_PATH.exists():\n",
        "        content_tail = log_tail(LOG_PATH, LOG_TAIL_LINES)\n",
        "\n",
        "        if \"Application startup complete.\" in content_tail:\n",
        "            print(\"‚úÖ vLLM SERVER STARTED and application startup complete.\")\n",
        "            print(f\"   Server is ready at: {LLM_SERVER_URL}\")\n",
        "            break\n",
        "\n",
        "        # Helpful debug: surface error if engine failed, but process is still alive for a bit\n",
        "        if \"Engine core initialisation failed.\" in content_tail:\n",
        "            print(\"‚ùå Detected 'Engine core initialisation failed' in logs.\")\n",
        "            print(\"\\nLast log lines:\\n\" + \"-\" * 60)\n",
        "            print(content_tail)\n",
        "            print(\"-\" * 60)\n",
        "            break\n",
        "\n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: If the vLLM server is already running from a previous cell execution, you can skip the startup cells above. Check the logs with `!tail -n 50 vllm.log` if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run SAM3 Agent Inference\n",
        "\n",
        "The agent logic runs locally, using:\n",
        "- **Local vLLM** for LLM calls\n",
        "- **Remote SAM3 endpoint** for SAM3 inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from IPython.display import display, Image\n",
        "from sam3.agent.client_llm import send_generate_request as send_generate_request_orig\n",
        "from sam3.agent.inference import run_single_image_inference\n",
        "\n",
        "# Create remote SAM3 client that calls Modal endpoint\n",
        "def call_sam_service_remote(\n",
        "    sam3_processor,  # Not used, but kept for interface compatibility\n",
        "    image_path: str,\n",
        "    text_prompt: str,\n",
        "    output_folder_path: str = \"sam3_output\",\n",
        "    deployment_url: str = SAM3_DEPLOYMENT_URL,\n",
        "):\n",
        "    \"\"\"\n",
        "    Remote version of call_sam_service that calls Modal SAM3 endpoint.\n",
        "    Matches the interface of the original call_sam_service function.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    from sam3.agent.client_sam3 import remove_overlapping_masks, visualize\n",
        "    \n",
        "    print(f\"üìû Loading image '{image_path}' and sending with prompt '{text_prompt}' to remote SAM3 endpoint...\")\n",
        "    \n",
        "    text_prompt_for_save_path = (\n",
        "        text_prompt.replace(\"/\", \"_\") if \"/\" in text_prompt else text_prompt\n",
        "    )\n",
        "    \n",
        "    os.makedirs(\n",
        "        os.path.join(output_folder_path, image_path.replace(\"/\", \"-\")), exist_ok=True\n",
        "    )\n",
        "    output_json_path = os.path.join(\n",
        "        output_folder_path,\n",
        "        image_path.replace(\"/\", \"-\"),\n",
        "        rf\"{text_prompt_for_save_path}.json\",\n",
        "    )\n",
        "    output_image_path = os.path.join(\n",
        "        output_folder_path,\n",
        "        image_path.replace(\"/\", \"-\"),\n",
        "        rf\"{text_prompt_for_save_path}.png\",\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        # Encode image to base64\n",
        "        with open(image_path, \"rb\") as f:\n",
        "            image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "        \n",
        "        # Call remote SAM3 endpoint\n",
        "        request_body = {\n",
        "            \"text_prompt\": text_prompt,\n",
        "            \"image_b64\": image_b64,\n",
        "        }\n",
        "        \n",
        "        response = requests.post(deployment_url, json=request_body, timeout=600)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        \n",
        "        if result.get(\"status\") != \"success\":\n",
        "            raise Exception(f\"SAM3 endpoint error: {result.get('message', 'Unknown error')}\")\n",
        "        \n",
        "        # Format response to match sam3_inference output\n",
        "        serialized_response = {\n",
        "            \"orig_img_h\": result[\"orig_img_h\"],\n",
        "            \"orig_img_w\": result[\"orig_img_w\"],\n",
        "            \"pred_boxes\": result[\"pred_boxes\"],\n",
        "            \"pred_masks\": result[\"pred_masks\"],\n",
        "            \"pred_scores\": result[\"pred_scores\"],\n",
        "        }\n",
        "        \n",
        "        # Apply same post-processing as original call_sam_service\n",
        "        serialized_response = remove_overlapping_masks(serialized_response)\n",
        "        serialized_response = {\n",
        "            \"original_image_path\": image_path,\n",
        "            \"output_image_path\": output_image_path,\n",
        "            **serialized_response,\n",
        "        }\n",
        "        \n",
        "        # Reorder predictions by scores (highest to lowest)\n",
        "        if \"pred_scores\" in serialized_response and serialized_response[\"pred_scores\"]:\n",
        "            score_indices = sorted(\n",
        "                range(len(serialized_response[\"pred_scores\"])),\n",
        "                key=lambda i: serialized_response[\"pred_scores\"][i],\n",
        "                reverse=True,\n",
        "            )\n",
        "            serialized_response[\"pred_scores\"] = [\n",
        "                serialized_response[\"pred_scores\"][i] for i in score_indices\n",
        "            ]\n",
        "            serialized_response[\"pred_boxes\"] = [\n",
        "                serialized_response[\"pred_boxes\"][i] for i in score_indices\n",
        "            ]\n",
        "            serialized_response[\"pred_masks\"] = [\n",
        "                serialized_response[\"pred_masks\"][i] for i in score_indices\n",
        "            ]\n",
        "        \n",
        "        # Remove invalid RLE masks\n",
        "        valid_masks = []\n",
        "        valid_boxes = []\n",
        "        valid_scores = []\n",
        "        for i, rle in enumerate(serialized_response[\"pred_masks\"]):\n",
        "            if len(rle) > 4:\n",
        "                valid_masks.append(rle)\n",
        "                valid_boxes.append(serialized_response[\"pred_boxes\"][i])\n",
        "                valid_scores.append(serialized_response[\"pred_scores\"][i])\n",
        "        serialized_response[\"pred_masks\"] = valid_masks\n",
        "        serialized_response[\"pred_boxes\"] = valid_boxes\n",
        "        serialized_response[\"pred_scores\"] = valid_scores\n",
        "        \n",
        "        # Save JSON\n",
        "        with open(output_json_path, \"w\") as f:\n",
        "            json.dump(serialized_response, f, indent=4)\n",
        "        print(f\"‚úÖ Raw JSON response saved to '{output_json_path}'\")\n",
        "        \n",
        "        # Render and save visualization\n",
        "        print(\"üîç Rendering visualizations on the image ...\")\n",
        "        viz_image = visualize(serialized_response)\n",
        "        os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
        "        viz_image.save(output_image_path)\n",
        "        print(\"‚úÖ Saved visualization at:\", output_image_path)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error calling remote SAM3 service: {e}\")\n",
        "        raise\n",
        "    \n",
        "    return output_json_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare input args and run single image inference\n",
        "image = \"assets/images/test_image.jpg\"\n",
        "prompt = \"the leftmost child wearing blue vest\"\n",
        "image = os.path.abspath(image)\n",
        "\n",
        "# Setup functions for agent inference\n",
        "send_generate_request = partial(\n",
        "    send_generate_request_orig, \n",
        "    server_url=LLM_SERVER_URL, \n",
        "    model=llm_config[\"model\"], \n",
        "    api_key=llm_config[\"api_key\"]\n",
        ")\n",
        "\n",
        "# Use remote SAM3 service instead of local processor\n",
        "call_sam_service = partial(\n",
        "    call_sam_service_remote,\n",
        "    sam3_processor=None,  # Not used for remote, but kept for interface\n",
        "    deployment_url=SAM3_DEPLOYMENT_URL,\n",
        ")\n",
        "\n",
        "print(f\"Image: {image}\")\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"LLM Server: {LLM_SERVER_URL}\")\n",
        "print(f\"SAM3 Endpoint: {SAM3_DEPLOYMENT_URL}\")\n",
        "print(\"\\nStarting agent inference...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run agent inference with local vLLM and remote SAM3\n",
        "output_image_path = run_single_image_inference(\n",
        "    image, prompt, llm_config, send_generate_request, call_sam_service, \n",
        "    debug=True, output_dir=\"agent_output\"\n",
        ")\n",
        "\n",
        "# display output\n",
        "if output_image_path is not None:\n",
        "    display(Image(filename=output_image_path))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
